# -*- coding: utf-8 -*-
"""DATA CLEAN MISSIG VALUES OUTLIERS TEMPLATE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bD2TUSh4Y3bLxQA2SXxwtWfl4feXBAuz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
from sklearn.impute import KNNImputer
from scipy import stats
import dc_toolkit as dc

"""# 1. Initial Data Import and Preliminary Exploration

## Data Import
"""

filepath = '/content/Ames_Housing_Data.csv'
df = dc.load_data(filepath)

"""## Basic Structure and Summary Statistics"""

dc.explore_data(df)

"""## Identify Missing Values"""

missing_summary = dc.missing_values_summary(df)

"""## Initial Visualization for Outliers and Distribution"""

dc.plot_distributions(df)

"""# 2. Categorical Data Discovery"""

data = df.select_dtypes(include='number')
data1 = data.iloc[:,:data.shape[1]//2]
data2 = data.iloc[:,data.shape[1]//2:]
print(data1.columns, data2.columns)
data1.hist(figsize=(15,15),bins=30);

category_counts_df, category_counts_list = dc.analyze_column_categories_auto(df)
print(category_counts_df)

"""## Categorical Data Type Conversion"""

df = dc.convert_to_categorical(df, category_counts_list)
df.info()

data = df.select_dtypes(include='number')
data.hist(figsize=(15,15),bins=30);

"""# 3. Missing Data

Identifying the pattern and extent of missingness in your data is crucial for choosing the appropriate method for handling missing values. The concepts of Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR) help in understanding the nature of missingness, which in turn guides the imputation strategy.

### Understanding MCAR, MAR, and MNAR

- **MCAR (Missing Completely At Random)**: The probability of being missing is the same for all observations. There is no relationship between the missingness of data and any values, observed or missing.
- **MAR (Missing At Random)**: The probability of being missing is the same only within groups defined by the observed data. There is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.
- **MNAR (Missing Not At Random)**: The probability of being missing varies for reasons that are unknown to us and is related to the missing data itself.

### Approaching the Analysis

To analyze the pattern of missingness, you can start with descriptive statistics and visualizations to infer if the data is MCAR, MAR, or MNAR. Directly testing for MAR or MNAR is challenging because it involves the missing data itself, but you can infer the likely scenario based on patterns and relationships in your data.

#### 1. Visualizing Missing Data Patterns

A good starting point is to visualize the pattern of missing data. This can sometimes give insights into the nature of the missing data.

#### 2. Analyzing Missing Data for MCAR

To statistically test if the data is MCAR, you can use Little's MCAR test, though it's more common in statistical software like R. In Python, this test isn't directly available in popular libraries like `scipy` or `statsmodels`, and implementing it from scratch is non-trivial and beyond the scope of typical data cleaning tasks. However, understanding the pattern visually and through logical deduction is often sufficient.

#### 3. Deductive Analysis for MAR vs. MNAR

For MAR vs. MNAR, you can perform a deductive analysis based on understanding the data generation process and looking for patterns that might suggest MAR or MNAR. For example:

- **Comparing distributions**: Compare the distributions of observed data for rows with missing vs. non-missing data. Significant differences might suggest MAR.
- **Correlation analysis**: Analyze if the missingness in one variable correlates with another observed variable, which might suggest MAR.


### Conclusion

The process of determining if data is MCAR, MAR, or MNAR often involves a combination of visualization, statistical tests (where applicable), and deductive reasoning based on your understanding of the data and its context. While Python provides excellent tools for visualization and analysis, the determination of missingness patterns relies heavily on the analyst's ability to interpret these patterns within the specific context of their data.

### Identify the Pattern and Extent of Missingness
"""

dc.plot_missing_data_matrix(df)
dc.plot_missing_data_heatmap(df)

"""Interpreting the plots generated by `missingno` for missing data analysis involves understanding what each visualization represents and how it can inform you about the pattern and nature of missingness in your dataset. Here's how to interpret the two types of plots you mentioned:

### 1. Missing Data Matrix (`msno.matrix`)

The missing data matrix provides a visual representation of the presence (or absence) of data. It's particularly useful for spotting patterns of missingness across your dataset.

- **How to Read**: In the matrix, each row represents a row in your dataset, and each column represents a variable. White lines indicate missing values, and colored lines (often dark) indicate non-missing values.
- **Interpretation**:
  - **Random White Spaces**: If the white spaces (missing values) appear randomly scattered throughout the data, it suggests that the data might be Missing Completely At Random (MCAR).
  - **Patterns or Blocks of White**: If you notice blocks or patterns of white spaces, such as entire sections of rows or columns being white, it suggests a systematic missingness. This could indicate Missing At Random (MAR), where the propensity for a data point to be missing is related to another observed variable, or Missing Not At Random (MNAR), where the missingness is related to the unobserved data itself.
  - **Strips of White**: Consistent strips of white across a specific column indicate that a particular variable has a lot of missing values. This can be useful for identifying variables that might require special attention for imputation or might be candidates for removal from your analysis.

### 2. Missing Data Heatmap (`msno.heatmap`)

The missing data heatmap shows the correlation of missingness between different variables. This can help identify if the presence or absence of data in one variable is related to the presence or absence of data in another.

- **How to Read**: In the heatmap, each cell represents the correlation between the missingness of two variables. A positive value indicates that when one variable is missing, the other is also likely to be missing, and vice versa. A negative value indicates that when one variable is missing, the other is likely to be present, and vice versa.
- **Interpretation**:
  - **Positive Correlation (Warm Colors)**: Indicates that the missingness of two variables is related. For example, if the heatmap shows a positive correlation between two variables, it suggests that when one variable is missing, the other is also likely to be missing. This can hint at MAR if the missingness is systematically related to observed data.
  - **Negative Correlation (Cool Colors)**: Indicates an inverse relationship in the pattern of missingness between two variables. For example, when one variable is missing, the other is less likely to be missing. This pattern is less common but can occur in certain datasets.
  - **No Correlation (Colors Close to Zero)**: Indicates no relationship in the missingness between two variables. This can be suggestive of MCAR, where the missingness of one variable does not inform about the missingness of another.

### General Tips for Interpretation

- **Context is Key**: The interpretation of these plots should always consider the context of your dataset and the nature of the variables. Sometimes, domain knowledge is crucial for understanding why data might be missing and how it affects your analysis.
- **Use in Combination**: These visualizations are most powerful when used together, along with other exploratory data analysis techniques. They can provide initial insights that guide further investigation into the nature of the missing data.
- **Further Investigation**: Observing patterns of missingness is just the first step. It's often necessary to perform additional analysis, such as investigating the relationship between variables with missing data and other observed variables, to understand the underlying mechanisms causing the data to be missing.

By carefully analyzing these plots, you can gain valuable insights into the patterns of missingness in your dataset, which can inform your strategy for data imputation or removal, and help ensure the robustness of your subsequent analyses.
"""

#drop columns missing more than 40% of the data
print(missing_summary[missing_summary['Percentage']>40])
miss_cols = missing_summary[missing_summary['Percentage']>40].index
df[miss_cols]

df.drop(columns=miss_cols, inplace=True)
dc.plot_missing_data_matrix(df)

missing_summary_2 = dc.missing_values_summary(df)

dc.analyze_missingness_correlation(df, 'Lot Frontage', 'SalePrice')

"""### Imputation vs. Deletion

When dealing with missing data, the choice between imputation and deletion largely depends on the nature and extent of the missingness, as well as the size of your dataset and the specific analysis or modeling you plan to perform. Here's a brief overview of common strategies for both imputation and deletion, followed by recommendations and Python code examples for imputation.

### Common Strategies for Handling Missing Data

#### Deletion

- **Listwise Deletion (Complete Case Analysis)**: Removing any record (row) that has any missing value. This is simple but can lead to significant data loss if missingness is widespread.
- **Pairwise Deletion**: Used mainly in statistical analyses, where calculations are performed only on available data. Not commonly recommended for predictive modeling due to potential biases.
- **Dropping Variables**: If a variable has a high percentage of missing values, and if it's not critical to your analysis or model, you might consider removing it entirely.

#### Imputation

- **Mean/Median/Mode Imputation**: Replacing missing values with the mean, median, or mode of the column. Simple and effective for numerical data but can reduce variability and affect the distribution.
- **Random Imputation**: Filling in missing values randomly selected from observed values in the same column. Preserves the distribution but not the relationships among variables.
- **K-Nearest Neighbors (KNN) Imputation**: Using the KNN algorithm to impute missing values based on the nearest neighbors. Good for datasets where data points form clusters.
- **Regression Imputation**: Predicting missing values using other variables in a regression model. Can be very effective but risks introducing bias if the model is not well-specified.
- **Iterative Imputation (Multivariate Imputation by Chained Equations, MICE)**: A more sophisticated approach that models each variable with missing values as a function of other variables in a round-robin fashion.

### Recommendations

- **For Numerical Data**: Mean/median imputation for data missing completely at random (MCAR) when the percentage of missing data is low. KNN or iterative imputation can be more appropriate for more complex patterns of missingness or when preserving relationships between variables is important.
- **For Categorical Data**: Mode imputation or using a placeholder value (e.g., "Unknown") can be effective. For more sophisticated approaches, KNN or iterative imputation with categorical support can be used.

### Conclusion

Choosing the right imputation method depends on your specific dataset and analysis goals. It's often useful to experiment with multiple approaches to see which works best for your situation. Remember, imputation introduces assumptions into your data, so it's crucial to understand these implications and consider them in your analysis or modeling efforts.
"""

# # Mean imputation
# df['column_name'].fillna(df['column_name'].mean(), inplace=True)

# # Median imputation
# df['column_name'].fillna(df['column_name'].median(), inplace=True)

# # Mode imputation (for categorical variables)
# df['column_name'].fillna(df['column_name'].mode()[0], inplace=True)

"""#### KNN Imputation Using Scikit-Learn"""

# from sklearn.impute import KNNImputer

# imputer = KNNImputer(n_neighbors=5)
# df_filled = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

"""#### Iterative Imputation (MICE) Using Scikit-Learn"""

# from sklearn.experimental import enable_iterative_imputer
# from sklearn.impute import IterativeImputer

# iterative_imputer = IterativeImputer()
# df_filled = pd.DataFrame(iterative_imputer.fit_transform(df), columns=df.columns)

"""#### Mode Imputation for Categorical Data"""

# df.select_dtypes(exclude='number').info()

dc.mode_imputation(df)

obs_cols = df.select_dtypes(exclude='number').columns
print('missing values in categorical data =',
      df[obs_cols].isna().sum().sum())

"""#### KNN Imputation for Numeric Variables

- **Choosing `n_neighbors`**: The `n_neighbors` parameter determines how many nearest neighbors to consider. The optimal number depends on your dataset; you might need to experiment to find the best value.
- **Weights**: The `weights` parameter can be set to `'uniform'` (all points in each neighborhood are weighted equally) or `'distance'` (weights points by the inverse of their distance, meaning nearer neighbors contribute more to the imputation).
- **Scaling**: KNN imputation can be sensitive to the scale of your features because it uses distance metrics to find the nearest neighbors. Consider standardizing your numeric data (to have mean = 0 and variance = 1) before applying KNN imputation, especially if the ranges of your numeric columns vary widely.

This approach allows you to specifically target numeric columns for KNN imputation, preserving the integrity of your categorical data while effectively handling missing values in your numerical data.
"""

from sklearn.impute import KNNImputer

numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
# Initialize the KNNImputer
imputer = KNNImputer(n_neighbors=5, weights='distance')
# Fit the imputer and transform the selected numeric columns, then update the DataFrame
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

print('Missing values in numeric columns =',
      df[numeric_cols].isna().sum().sum())

# final check
dc.plot_missing_data_matrix(df)

"""# 3. Outliers

After imputing both categorical and numeric data, the next logical step in your data preprocessing pipeline is to detect and handle outliers. Outliers can significantly affect your statistical analyses and predictive modeling by potentially skewing your data's distribution, mean, and standard deviation. There are several methods to detect outliers, and the choice of method can depend on the nature of your data and the specific requirements of your analysis or modeling task.

### Outlier Detection Methods

1. **Statistical Methods**:
   - **Z-Score**: A Z-score measures the number of standard deviations an observation is from the mean. Observations with a Z-score that exceeds a certain threshold (commonly 3 or -3) are considered outliers.
   - **IQR (Interquartile Range) Score**: The IQR is the difference between the 75th and 25th percentiles of the data. Observations that fall below the 25th percentile - 1.5*IQR or above the 75th percentile + 1.5*IQR are considered outliers.

2. **Visualization Techniques**:
   - **Box Plots**: A box plot is a standardized way of displaying the distribution of data based on a five-number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can help to visually identify outliers.
   - **Scatter Plots**: Useful especially for multivariate data, where you're looking to find outliers with respect to two variables.

3. **Machine Learning Methods**:
   - **Isolation Forest**: An algorithm that isolates anomalies instead of profiling normal data points. It's effective for high-dimensional datasets.
   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: A clustering algorithm that separates high-density areas from low-density areas, identifying outliers as points in low-density regions.

### Recommendations for Starting with Outlier Detection

1. **Understand Your Data**: Before applying any method, it's crucial to understand your data. Some "outliers" might be genuine values that are essential to your analysis.

2. **Use Statistical Methods for Univariate Analysis**: Start with Z-score or IQR methods for univariate outlier detection. These methods are straightforward and effective for initial analyses.

3. **Visualize Your Data**: Use box plots and scatter plots to visually inspect for outliers. This can also help in multivariate outlier detection.

4. **Consider the Context**: Decide on the importance of detected outliers. In some cases, removal might be the best option, while in others, transformation or simply keeping them might be more appropriate.

### Conclusion

Outlier detection and handling is a nuanced process that requires a good understanding of your data and the context of your analysis. After detecting outliers, consider the best approach for handling them based on your specific needs—whether it's removal, transformation, or further investigation. Remember, the goal is to ensure that your data analysis or modeling is not unduly influenced by extreme values, while also preserving the integrity and representativeness of your data.
"""

# Q1 = df['column_name'].quantile(0.25)
# Q3 = df['column_name'].quantile(0.75)
# IQR = Q3 - Q1

# # Define bounds for outliers
# lower_bound = Q1 - 1.5 * IQR
# upper_bound = Q3 + 1.5 * IQR

# # Filter out outliers
# filtered_df = df[(df['column_name'] >= lower_bound) & (df['column_name'] <= upper_bound)]

"""### Detection"""

outliers_dict = dc.detect_outliers_iqr(df)

# Print or analyze the outliers
for column, outliers in outliers_dict.items():
    print(f"Outliers in column {column}:")
    print(outliers, "\n")

outliers_dict.keys()

outliers_dict['Lot Frontage']

"""### Visualize Outliers"""

summary_df = dc.visualize_and_summarize_outliers(outliers_dict, df)
print(summary_df)

"""### Outlier Assessment
Before moving on to removing outliers, it's important to ensure that you've conducted a thorough analysis and considered the implications of outlier removal on your dataset and subsequent analyses or modeling efforts. Here are a few considerations and steps you might want to take before deciding to remove outliers:

### 1. **Understand the Nature of Outliers**

- **Source of Outliers**: Determine whether outliers are due to data entry errors, measurement errors, or if they are natural variations within the data. Understanding the source can guide whether you should correct, remove, or retain these outliers.
- **Impact on Analysis**: Consider how outliers might affect your analysis or modeling. In some cases, outliers can reveal valuable insights or indicate important phenomena.

### 2. **Consult Domain Knowledge**

- **Expert Opinion**: If possible, consult with domain experts to understand whether outliers represent rare but important cases that should be included in your analysis.
- **Relevance to Objective**: Consider the relevance of outliers to your analysis objectives. Sometimes, what appears as an outlier might be crucial for specific research questions or business objectives.

### 3. **Consider Alternative Treatments for Outliers**

- **Transformation**: Instead of removal, consider transforming the data. Log transformation, square root transformation, or other non-linear transformations can sometimes reduce the impact of outliers.
- **Winsorization**: Capping the outliers at a certain percentile of the data (e.g., the 1st and 99th percentiles) can minimize their effect without completely removing them.
- **Separate Analysis**: In some cases, it might be useful to analyze outliers separately to understand their characteristics or to build specialized models.

### 4. **Assess the Proportion of Data Affected**

- **Data Loss**: Evaluate how much data you would lose by removing outliers. Significant data loss might bias your results or reduce the statistical power of your analyses.
- **Distributional Impact**: Consider how removal will affect the distribution of your data. Removing outliers can sometimes lead to changes in assumptions underlying your statistical tests or models.

### 5. **Perform Sensitivity Analysis**

- **With vs. Without Outliers**: Before finalizing the removal, perform a sensitivity analysis to understand how your results change with and without the outliers. This can help in assessing the robustness of your findings.

### 6. **Document Your Decisions**

- **Rationale for Removal**: Document the reasons for removing any data points, including the criteria used for defining them as outliers and the methods used for their removal or treatment.
- **Impact Analysis**: Keep a record of how the removal of outliers affects your analyses or models. This documentation is crucial for transparency and reproducibility.

### Moving Forward

If, after careful consideration, you decide that removing outliers is justified and necessary for your analysis, proceed with the removal. However, ensure that this step is well-documented and that you've considered the potential impacts on your analysis. Remember, the goal is to ensure the integrity and validity of your findings, and sometimes retaining outliers with appropriate adjustments or transformations can be more informative than removing them outright.

### Outlier Treatment and Removal

Removing outliers is a critical step that should be approached with caution, as it can significantly impact the results of your analysis or modeling. The best approach to removing outliers often depends on the nature of your data, the reasons why certain data points have been identified as outliers, and the specific requirements of your analysis or modeling task. Here are some recommended strategies for removing outliers:

### 1. **Using IQR (Interquartile Range) Method**

The IQR method is widely used for outlier detection and removal. It involves calculating the IQR (Q3 - Q1) and then determining the lower and upper bounds beyond which data points are considered outliers. Here's how you can remove outliers based on the IQR method:

```python
def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Only keep rows in dataframe that do not contain any outliers
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
```

### 2. **Using Z-Score Method**

The Z-score method involves calculating the Z-score for each data point, which measures how many standard deviations away a data point is from the mean. Data points with a Z-score beyond a certain threshold (commonly 3 or -3) are considered outliers.

```python
from scipy import stats

def remove_outliers_zscore(df, column):
    df = df[(np.abs(stats.zscore(df[column])) < 3)]
    return df
```

### 3. **Condition-Based Removal**

If you have domain knowledge that specifies what constitutes an outlier, you can remove outliers based on specific conditions. For example, if you know that values above or below certain thresholds are impossible or erroneous:

```python
def remove_outliers_condition(df, column, min_val, max_val):
    return df[(df[column] >= min_val) & (df[column] <= max_val)]
```

### 4. **Using Automated Libraries**

Libraries like `scikit-learn` offer more sophisticated methods for outlier detection and removal, such as `IsolationForest` or `LocalOutlierFactor`. These can be particularly useful for multivariate datasets where outliers need to be detected in a multi-dimensional feature space.

### Recommendations:

- **Backup Your Data**: Always make a copy of your data before removing outliers. This allows you to revert changes if needed.
- **Consider the Context**: Always consider the context and the impact of removing outliers. In some cases, it might be more appropriate to correct outliers rather than remove them, especially if they represent valuable information.
- **Document the Process**: Keep detailed documentation of the criteria and process used for identifying and removing outliers. This is crucial for the reproducibility of your analysis.
"""

df[df.select_dtypes(include=['float64', 'int64']).columns].hist(figsize=(15,15),bins=30);

# create a copy of the original df
original_df = df.copy()
# Apply to your dataframe
cleaned_df = dc.remove_outliers_iqr_all(df)

cleaned_df.select_dtypes(include=['float64', 'int64']).hist(figsize=(15,15),bins=30);

# cleaned_df['Enclosed Porch'].hist(bins=30);
# original_df['Enclosed Porch'].hist(bins=30)
cleaned_df['Screen Porch'].unique()

# cols = category_counts_df[category_counts_df['Unique_Values']<20]['Column'].tolist()
# cleaned_df[cols]

print(original_df.shape, cleaned_df.shape)

def sum_outliers(outliers_dict):
    total_outliers = 0
    for column, outliers_df in outliers_dict.items():
        total_outliers += len(outliers_df)
    return total_outliers

# Assuming outliers_dict is the output from your detect_outliers_iqr function
outliers_dict = dc.detect_outliers_iqr(df)
total_outliers = sum_outliers(outliers_dict)
print(f"Total number of outliers across all columns: {total_outliers}")
print(f"Total number of rows in original dataframe: {original_df.shape[0]}")

# create copy of original data frame
ogdf = original_df.copy()
print(ogdf.shape)

# reduced outlier removal function
outlier_indices = set()
for column in numeric_cols:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Find indices of outliers in the current column
    column_outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].index
    outlier_indices.update(column_outliers)

# Remove rows with outliers in any column
df_filtered = ogdf.drop(index=outlier_indices)
print(df_filtered.shape)

"""# 4. Exploratory Data Analysis (EDA)"""



